# 22. Swapping - 교체 정책(Policies)

빈 메모리 공간이 없을시 운영체제는 메모리 압박(memory pressure)을 받는다. 이를 해소하기 위해 덜 사용되는 페이지들을 **paging out** 하여 **활발히 사용 중인 페이지들을 위한 공간을 확보** 한다.

**내보낼(evict)** 페이지들의 선택하는 메커니즘을 운영체제의 **교체 정책(replacement policy)** 이라 한다.

운영체제가 내보낼 페이지를 어떻게 결정하는지 교체 정책에 대해 다뤄본다.

---

## 22.1 캐시 관리

시스템의 전체 페이지들 중 일부만이 메인 메모리에 유지된다는 개념에서, _메인 메모리는 시스템의 가상 메모리 페이지를 가져다 놓기 위한 **캐시**_ 로 생각될 수 있다.

따라서 **교체 정책의 목표는 캐시 미스의 횟수를 최소화하여 디스크로부터 페이지를 가져오는 횟수를 최소로 만드는 것** 이다. 같은 개념으로 **캐시 히트의 횟수를 최대화하여 접근된 페이지가 메모리에 이미 존재하는 횟수를 최대로 하는것** 이다.

<br>

교체 정책간의 비교의 잣대로 **평균 메모리 접근 시간(AMAT, average memory access time)** 이 있다.

![img4](https://user-images.githubusercontent.com/35681772/59990795-8ea98c80-967f-11e9-91f1-03a5705bc6c7.png)

  - TM : 메모리 접근 비용
  - TD : 디스크 접근 비용
  - PH : 캐시에서 데이터를 찾을 확률(히트)
  - PM : 캐시에서 데이터를 찾지 못할 확률(미스)

여기서 PH와 PM은 확률이기 때문에 0.0에서 1.0 사이의 값을 가지며 PH + PM = 1.0을 만족한다.

예시는 아래와 같다.

어떤 컴퓨터가 4KB의 작은 메모리를 갖고 있고, 페이지의 크기는 256Byte인 경우 메모리는 4KB = (2^2) * (2^10) = 2^12Byte다. 따라서 페이지의 크기인 256 = 2^8Byte로 나누게 되면 페이지의 개수는 2^4 총 16개의 가상 페이지에 접근할 수 있다.

각 가상 주소들의 첫 16진수 숫자가 페이지 번호를 나타낸다면, 프로세스는 가상 주소 0x000, 0x100, 0x200, 0x300, ... , 0x900의 메모리를 가리키고 있다. 이 가상 주소들은 주소 공간의 첫 열 개 페이지들의 첫 번째 바이트를 나타낸다.

가상 페이지 3을 제외한 모든 페이지가 이미 메모리에 있는 경우, 메모리 참조는 다음과 같을 것이다 : 히트, 히트, 히트, 미스, 히트, 히트, 히트, 히트, 히트, 히트.

여기서 _히트율(hit rate)_ 은 10개 중 9개의 히트가 발생하였으므로 90%가 된다(PH = 0.9). _미스율(miss rate)_ 은 10%(PM = 0.1)이 된다.

AMAT를 계산하기 위해선 메모리를 접근하는 비용과 디스크를 접근하는 비용을 알아야 한다. 메모리를 접근하는 비용(TM)이 약 100ns이고, 디스크를 접근하는 비용(TD)이 10ms일 때, AMAT은 __AMAT = (0.9)*100ns + (0.1)*10ms = 0.9ns + 1ms = 1.0009ms 약 1ms가 된다.__

여기서 만약 히트율이 99.9%라면 결과가 큰 차이로 달라지게 되는데, **PH = 0.999일 때 AMAT은 10.1ms**가 된다. 히트율이 100%에 가까워질수록 AMAT은 100ns에 가까워진다.

위 예를 통해 현대 시스템에서는 **디스크 접근 비용이 너무 크기 때문에 아주 작은 미스가 발생하더라도 전체 AMAT에 큰 영향을 주게 된다** 는 것을 알 수 있다. 그러므로 미스를 최대한 줄여야 한다.

따라서 어떤 정책을 사용하여 미스율을 낮출지 알아보자.

---

## 22.2 최적 교체 정책

![img5](https://user-images.githubusercontent.com/35681772/59990801-98cb8b00-967f-11e9-882f-8b6b768bdeca.png)

프로그램이 0 → 1 → 2 → 0 → 1 → 3 → 0 → 3 → 1 → 2 → 1 순으로 가상 페이지들을 접근한다고 할 경우, 최적 교체 정책은 위와같이 진행된다. 캐시에는 세 개의 페이지를 저장할 수 있다고 가정한다.

캐시는 최초 비어있는 상태로 시작하기 때문에 처음 세번의 접근에 미스가 난다. 이를 **최초 시작 미스(cold-start miss)** 또는 **강제 미스(compulsory miss)** 라고 부른다. 

그 뒤 0과 1의 접근에서 이미 캐시에 존재하기 때문에 히트가 나고 그 다음으로 현재 캐시에 탑재되어 있지 않은 3을 access하게 된다.

이 경우 캐시의 capacity가 3개이기 때문에 가득 찬 상황이라 하나를 내보내고 3을 캐시에 탑재해야 한다.

현 시점으로부터 가장 나중에 접근을 필요로하는(즉, 미래의 일을 확인하여서) 2를 evict하고 3을 캐시 메모리에 탑재한다.

이후 히트를 거쳐 아까 내보냈던 2에대한 access시에 다시한번 미래의 access들을 확인하여 어느것을 내보낼지 결정하게 된다. 이 시점에선 바로 다음에 접근하는 1을 제외한 어떤것을 내보내도 그 이후에 종료되기 때문에 상관이 없다. 

이 예시에서의 캐시 히트율은 Hits/(Hits + Misses) 로 계산되어 6/(6 + 5) = 54.6% 이다.

그러나 이 최적 교체 정책은 '미래의 접근을 가지고 판단'하기 때문에 구현이 불가능하다. 하지만 이 정책이 최적이기 때문에, 이를 잣대로 다른 정책들이 얼만큼 최적에 가까워서 더 좋은지 / 나쁜지를 판단하는 척도가 된다.

---

## 22.3 간단한 정책 : FIFO

![img6](https://user-images.githubusercontent.com/35681772/59990803-9f5a0280-967f-11e9-83bc-ed47dbce629c.png)

FIFO 정책은 말그대로 선입선출을 따르는 전형적인 queue 방식의 정책이다. 교체할 상황이 오면 큐에 가장 먼저 들어온 페이지가 내보내진다. 흐름은 위와 같이 나타난다.

최적의 경우와 비교했을 때 FIFO는 히트율이 36.4%로 최적에 비해 눈에 띄게 히트율이 낮은것을 확인할 수 있다.

FIFO는 페이지들의 중요도를 판단하지 않아, 페이지 0이 여러 차례 접근이 되었더라도 단순히 메모리에 먼저 들어왔다는 이유로 FIFO는 페이지 0을 내보내는 식이다.

![img7](https://user-images.githubusercontent.com/35681772/59990808-a4b74d00-967f-11e9-9d53-7ffb92c57457.png)

또한 Belady가 FIFO정책에 관해 연구하던 중, 캐시 메모리의 크기가 커지면 페이지 폴트 발생률이 낮아져야 하는데(크기가 커져서 캐시 메모리에 탑재 가능한 페이지 capacity가 늘어나므로) FIFO 정책의 일정 구간에서 이를 역행한다는 것을 보였다. 이를 Belady's Anomaly라고 한다.

---

## 22.4 간단한 정책 : RANDOM

![img8](https://user-images.githubusercontent.com/35681772/59990818-ac76f180-967f-11e9-97df-7bed213c4717.png)

![img9](https://user-images.githubusercontent.com/35681772/59990822-b1d43c00-967f-11e9-814d-573579cb8533.png)

캐시 메모리에서 Random하게 선택하여 페이지를 evict 하는 방식이다. FIFO와 마찬가지로 구현하기 쉽지만 내보낼 블럭을 제대로 선택하는 방식은 아니다.

FIFO보다는 약간 더 좋은 성능을 보이지만, stability가 떨어져서 전적으로 운에 따라 히트율이 달라지게 된다.

---

## 22.5 과거 정보의 사용 : LRU(Least-Recently-Used)

![img3](https://user-images.githubusercontent.com/35681772/59990828-b862b380-967f-11e9-86f5-6a958a4a0d05.jpg)

미래에 대한 예측을 위해 _과거 사용 이력_ 을 반영해본다. 어떤 프로그램이 가까운 과거에 한 페이지를 접근했다면 가까운 미래에도 그 페이지를 다시 접근하게 될 확률이 높다는 것.

이는 **지역성의 원칙(principle of locality)** 을 정책에 반영한 것인데, 지역성의 원칙은 다음과 같다.
  - **공간 지역성(spatial locality)** : 어떤 페이지 P가 접근되었다면, 그 페이지 주변의 페이지들(P-1, P+1 등)이 참조되는 경향이 있다는 것
  - **시간 지역성(temporal locality)** : 가까운 과거에 참조되었던 페이지는 가까운 미래에 다시 접근되는 경향이 있다는 것


이 개념을 swapping policy에선 아래와 같이 반영한다.

  - **빈도수(Frequency)** : 만약 한 페이지가 여러 차례 접근되었다면, 다시 접근될 가능성이 높을 것이다.
  - **최근성(Recency)** : 더 최근에 접근된 페이지일수록 가까운 미래에 접근될 확률이 높을 것이다.

이러한 지역성이 적극 반영되는 예시로 코드부분에서는 반복문 코드가 있고, 자료 구조에서는 그 반복문에 의해 접근되는 배열을 예로 들을 수 있다.

이렇게 과거를 반영한 교체 알고리즘으로는 **LFU(Least-Frequently-Used)** 와 **LRU(Least-Recently-Used)** 가 있다.

**LFU(Least-Frequently-Used)** : 가장 적은 빈도로 사용된 페이지를 교체한다.

**LRU(Least-Recently-Used)** : 가장 오래 전에 사용하였던 페이지를 교체한다.

위 예시에서 확인할 수 있듯, LRU에선 Most Recently Used 항목을 매 Access마다 갱신한다. 따라서 교체가 일어날 때 Least Recently Used 항목을 교체 대상으로 선택하면 된다. 

3에 대한 교체가 일어날 때, Least Recently accessed 항목은 2 이므로 2를 evict 한다.

이 예제만 놓고 보면 최적 기법과 같은 정도 수준의 성능을 보여주고 있다.

---

## 22.6 워크로드에 따른 성능 비교(Workload Examples)

먼저 지역성을 고려하지 않은 완전히 randomly access하는 예시를 보자.

![img11](https://user-images.githubusercontent.com/35681772/59990838-c1ec1b80-967f-11e9-9269-c8d3376242ff.png)

지역성이 전혀 고려되지 않은 상황 즉, page access pattern을 고려하지 않은 상황에서는 아래와 같은 결론을 얻을 수 있다.

  - 첫째, 어떤 정책을 사용하여도 모두 동일한 성능을 보이며, 히트율은 캐시의 크기에 의해 결정된다.
  - 둘째, 캐시가 충분히 커서 모든 워크로드를 다 포함할 수 있다면 역시 어느 정책을 사용하던 상관이 없다.
  - 셋째, 최적 기법이 구현 가능한 기타 정책들보다 현저하게 좋은 성능을 보인다.

---

다음으로 80-20 워크로드에선 아래와 같은 결과를 보인다.

![img12](https://user-images.githubusercontent.com/35681772/59990842-c87a9300-967f-11e9-9042-0c57b3ff81ee.png)

80-20 워크로드는 20%의 인기있는 페이지(hot pages)에서 80%의 참조가 발생하고, 나머지 80%의 페이지들(cold pages)에서 20%의 참조만 발생한다는 개념이다.

이 경우는 page access pattern이 고려된 경우로서, LRU에선 인기있는 페이지(hot pages)들을 캐시에 더 오래두는 경향이 있기 때문에 더 좋은 성능을 보이고 있다.

---

마지막으로 순차 반복(looping sequential) 워크로드에선 아래와 같은 양상을 보인다.

![img13](https://user-images.githubusercontent.com/35681772/59990845-cdd7dd80-967f-11e9-8b10-801b3b4145a2.png)

순차 반복 워크로드는 50개의 페이지들을 순차적으로 반복하여 접근하는 것을 말하고, 이 테스트에서 10000번의 접근이 일어났다.

1번, 2번, 3번, ..., 49번, 50번, 1번, ... 와 같은 순서로 반복적으로 10000회 접근을 시도한다는 것이다.

이 상황에서는 LRU와 FIFO에서 가장 안좋은 성능을 보이게 된다. Least Recently Used 순서를 유지하게 되므로, 특정 페이지에 대해 재참조가 일어나기 직전에 페이지에서 swap out 해 버리게 되어 위와같이 LRU와 FIFO는 worst한 성능을 보이게 된다.

여기서 확인할 수 있는 사실 중 하나는, 캐시 사이즈가 커지게 되면 input sequence에 무관하게 모두 hit이 발생하게 된다는 것이다. 50개의 페이지들을 대상으로 하였으므로 캐시가 이 50개의 페이지를 모두 담는 시점부턴 알고리즘간 비교가 무의미해진다.

---

## 22.7 과거 기반 알고리즘의 구현(Implementing Historical Algorithms)

LRU와 같이 과거 이력을 기반으로 한 알고리즘을 구현하는데엔 다음과 같은 문제점들이 존재한다.
  
먼저, **각 페이지 접근마다 해당 페이지가 리스트에 가장 앞으로 이동하도록(즉, Most-Recently-Used) 자료 구조를 갱신해야 한다.**

이것을 가능하게 하려면 어떤 페이지가 가장 최근에 또는 가장 오래 전에 사용되었는지를 관리하기 위해 **모든 메모리 참조 정보를 기록해야 한다.**

이를 하드웨어의 도움을 받아 기록까지 가능케 되었더라도, 페이지 교체시 운영체제는 가장 오래 전에 사용된 페이지를 찾기 위해 시간 필드를 전체 탐색해야 한다.

이는 _엄청나게 고비용의 연산을 필요로 하는것으로, 완벽한 LRU를 구현하는 데에 무리가 생긴다._ 따라서 **LRU를 근사** 하는 방법을 알아본다.

---

## 22.8 LRU 정책 근사(Approximating LRU)

LRU 정책을 근사하기 위해선 하드웨어의 도움을 받아야 한다. **use bit(= reference bit)** 을 추가적으로 둬서, _페이지가 참조될 때마다 하드웨어에 의해서 1로 설정_ 된다. 반면 _0으로의 설정은 운영체제에 의해서 설정_ 된다.

LRU를 근사한 대표적인 알고리즘으로 **clock algorithm** 이 있다.

![img14](https://user-images.githubusercontent.com/35681772/59990851-d5978200-967f-11e9-87c0-cd1b37d7d25c.png)

clock algorithm은 환형 리스트로 구현되어, 시계 바늘(clock hand)이 특정 페이지를 가리키게 된다. 그 시계 바늘이 계속 빙빙 돌면서 페이지들의 use bit을 체크한다. 그래서 그 페이지의 use bit이 1이라면 해당 페이지 P는 최근에 참조된 것이므로 운영체제가 이 bit을 0으로 바꾸고 그냥 넘어간다. 하지만 만약 0이라면 evict 하고 다음 페이지로 넘어간다.

다시말해 시계 바늘이 한바퀴를 돌면서 1인 use bit을 모두 0으로 바꿨는데, 다음 한바퀴를 더 돌며 이전에 0으로 바꾼 페이지가 다시 1이 되어있다면 그 페이지는 최근에 참조가 된 것이다. 따라서 다시 0으로 바꾼 뒤 넘어가는 것이고, 0으로 바꾼 페이지가 0으로 남아있다면 시계 바늘이 한바퀴 도는동안 참조되지 않은 것이므로 evict하게 된다.

<br>

![img15](https://user-images.githubusercontent.com/35681772/59990856-daf4cc80-967f-11e9-9c32-cc8530d34d19.png)

위 그래프는 approximated LRU인 clock algorithm의 workload를 보여준다. 
완벽한 LRU에 비해선 퍼포먼스가 약간 떨어지지만 cost 측면에서 꽤 괜찮은 그래프를 그리고 있다.

---

## 22.9 갱신된 페이지(Dirty page)의 고려

운영체제가 교체 대상을 선정할 때 해당 페이지가 modified 되었는지를 고려기준에 포함하는 것이다. 만약 modified page라면, 그냥 evict 해서는 안되고 I/O를 통해 디스크에 변경된 내용을 기록해야 하므로, 변경되지 않은 상태(clean)에 비해 추가적인 오버헤드가 발생한다.

따라서 **특정 페이지의 modified 여부를 체크하는 modified bit(= dirty bit)** 을 추가적으로 둬서 교체 대상을 선정할때 이것이 함께 고려된다. 

그 예시로 clock algorithm은 _참조되지 않았고(reference bit = 0), clean한(dirty bit = 0)_ 페이지를 더 먼저 evict하게 된다.

---

## 22.10 다른 VM 정책들

_페이지 선택(page selection) 정책_ : 운영체제가 언제 페이지를 메모리로 불러들일지를 결정하는 것

_요구 페이징(demanding paging) 정책_ : 페이지를 읽어 들일 때의 정책. 요청된 후 즉시 운영체제가 해당 페이지를 메모리로 읽어 들이는 것

![img16](https://user-images.githubusercontent.com/35681772/59990871-e3e59e00-967f-11e9-8c2e-1826dbe738d0.png)

**_선반입(prefetching)_** : 미리 메모리로 읽어 들이는 것. spatial을 고려하여 그 인근 page도 사용될 가능성이 높다고 추측(guess)하여 미리 가져오는 것.

![img17](https://user-images.githubusercontent.com/35681772/59990880-eb0cac00-967f-11e9-940b-c3ad7ea22151.png)

**_clustering 또는 grouping of writes_** : 기록해야 할 페이지들을 여러 개의 작은 크기가 아니라 메모리에 모은 후, 한 번에 디스크에 기록하는 것. 디스크의 특성상 single large write이 더 효율적으로 쓰기 요청을 처리한다.

---

## 22.11 쓰래싱(Thrashing) : 시스템이 끊임없이 페이징을 하는 것

![img18](https://user-images.githubusercontent.com/35681772/59990887-f069f680-967f-11e9-9738-cfb63fbfb66d.png)

메모리 사용 요청이 많아져 페이지 공간이 부족하게 될 경우, 페이지 폴트가 발생하기 시작한다. 따라서 이 페이지 폴트를 처리하기 위해 I/O request가 발생하게 되는데, 그렇게 되면 CPU 이용률이 낮아진다.

이를 모니터링하던 **OS는 CPU의 이용률을 높히고자 더 많은 프로세스를 탑재하게 되고, 사실 쓰레싱으로 인한 I/O처리 때문에 CPU 이용률이 낮아진건데 더 많은 프로세스가 탑재되어 page-fault rate이 더 높아지게 된다.** 악순환이 일어나는 것.

이와 같이, **쓰레싱(Thrashing)은 메모리 사용 요구가 감당할 수 없을 만큼 많고, 실행중인 프로세스가 요구하는 메모리가 가용 물리 메모리의 크기를 초과하는 경우 운영체제가 끊임없이 페이징을 하는 것** 을 말한다. 위의 그림 참고.

이를 해결하는 방법으로는 실행되는 프로세스의 수를 줄여 진입 제어(admission control)를 하는 방법이 있다. 더 적은 일을 제대로 하는 것이다.

또 다른 해결책으로 out-of-memory killer가 있다. 이 데몬은 많은 메모리를 요구하는 프로세스를 골라 죽이는 것이다. 하지만 떄로 서버를 죽여버리게 되는 상황을 직면할 수 있어 문제가 될 수 있다.

---

## 22.12 요약

현대 시스템들은 시계 알고리즘과 같은 LRU 근사 방법에 몇가지 성능 향상을 위한 기능을 추가하였다. 그 예로 _탐색 내성(scan resistance)_ 같은 것들이 있는데, LRU가 최악의 경우에 보이는 행동들을 방지하기 위한 것 들이다.

현대에 이르러 memory access 시간과 disk access 시간 사이의 gap이 점점 증가하고 있어, 디스크에 페이징 하는 비용이 점점 증가하고 있다. 따라서 앞서 언급한 알고리즘을 사용함에도 불구하고 빈번한 페이징을 감당할 수 없어 더 많은 메모리를 사용하는 쪽으로 흘러가고 있다.