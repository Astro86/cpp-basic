# 28. 락 (Locks)

Concurrency 즉, 병행성에 대한 문제를 락을 통해 해결하는 방법을 알아본다.

소스 코드의 critical section을 락으로 둘러 싸서, 그 critical section이 마치 하나의 원자 단위 명령어(single atomic instruction)인것처럼 실행되도록 한다.

---

## 28.1 락 : 기본 개념 (Locks: The Basic Idea)

![img1](https://user-images.githubusercontent.com/35681772/59991338-fc56b800-9681-11e9-95e9-6044cee976e0.png)

락은 하나의 변수이므로 락을 사용하기 위해서는 락 변수를 먼저 선언해야 한다. 위의 mutex가 락 변수에 해당된다.

락 변수는 **사용 가능(available, unlocked, free)** 상태 이거나, **사용 중(acquired)** 상태 이렇게 둘 중 하나의 상태를 갖는다.

critical section에서 정확히 하나의 쓰레드가 락을 획득하도록 구현하여야 한다.

lock() 루틴 호출을 통해 락 획득을 시도하고, 만약 어떤 쓰레드도 락을 갖고 있지 않으면 그 쓰레드는 락을 획득하여 critical section 내로 진입한다. 이렇게 진입한 쓰레드를 락 owner 라고 부른다.

한 쓰레드가 락을 획득한 상황에서 또 다른 쓰레드가 lock()을 호출(위 예제에선 mutex 변수)한다면, lock() 함수가 리턴하지 않는다. 이런 방식으로 락을 보유한 쓰레드가 critical section 내에 진입한 상태에서는 다른 쓰레드들이 진입할 수 없다.

락 owner가 unlock()을 호출한다면 락은 이제 다시 사용 가능한 상태(available)가 된다.

lock() 을 호출하였으나 당시 락이 acquired 상태여서 대기 중이던 쓰레드는 락의 상태가 변경되었다는 것을 정보를 받아 인지하고 락을 획득하여 critical section 내로 진입하게 된다.

쓰레드는 프로그래머가 생성하지만 운영체제가 제어한다. 하지만 **락은 프로그래머에게 스케줄링에 대한 최소한의 제어권을 제공한다.** 락으로 코드를 감싸서 그 코드 내에서는 하나의 쓰레드만 동작하도록 보장할 수 있다.

---

## 28.2 Pthread 락 (Pthread Locks)

락은 쓰레드 간에 상호 배제(mutual exclusion) 기능을 제공하기 때문에 POSIX 라이브러리 에서 mutex 라고 부른다. mutual exclusion은 한 쓰레드가 critical section 내에 있다면 이 쓰레드의 동작이 끝날 때까지 다른 쓰레드가 이 critical section 내에 들어 올 수 없도록 제한하는 방법을 가리킨다.

![img3](https://user-images.githubusercontent.com/35681772/59991346-04aef300-9682-11e9-8cd7-159c4917f67d.png)

POSIX 방식에서는 변수 명을 지정하여(위 예시에서 lock) 락과 언락 함수에 전달하고 있다. 

이렇게 함으로써 **하나의 락이 각기 다른 critical section의 모든 락을 담당하는 것이 아니라(_coarse-grained_ locking strategy)** , **각 critical section 마다 새로 락 변수를 선언하여 사용(_fine-grained_ locking strategy) 가능하도록** 한다.

---

## 28.3 락 구현 (Building A Lock)

효율적으로 락은 낮은 비용으로 mutual exclusion을 제공해야 한다. 이를 위해 하드웨어와 OS의 지원이 필요하다.

---

## 28.4 락의 평가 (Evaluating Locks)

락의 평가 항목으로는 다음과 같은 것들이 있다.

  - 상호 배제(mutual exclusion)의 **정확성(correctness)** : 락이 동작하여 critical section 내로 다수의 쓰레드가 진입하는 것을 막을 수 있는지에 대한 것.

  - **공평성(fairness)** : 쓰레드 간 락 획득에 대한 공평한 기회가 주어지는지에 대한 것. 공평하지 않으면 굶주리는(starve) 쓰레드가 발생할 수 있다.

  - **성능(performance)** : 락 사용 시간에 따른 오버헤드를 평가하는 것. 
    - 경쟁이 전혀 없는 하나의 쓰레드가 락을 획득하고 반납하는 과정에서의 성능
    - 다중 쓰레드가 단일 CPU 상에서 락을 획득하려고 경쟁할 때의 성능
    - 다중 쓰레드가 멀티 CPU 상황에서 락 경쟁 시의 성능

    이렇게 3가지를 비교해 봐야 한다.

---

## 28.5 인터럽트 제어 (Controlling Interrupts)

![img4](https://user-images.githubusercontent.com/35681772/59991357-0d9fc480-9682-11e9-9cc1-c9dd6ed492d6.png)

초창기 단일 프로세스 시스템에서는 mutual exclusion 지원을 위해 위와 같이 critical section 내에서는 인터럽트를 비활성화하는 방법을 사용했었다.

인터럽트가 비활성화 되기 때문에 critical section 내의 코드들이 원자적으로(atomically) 실행될 수 있다. 하지만 아래와 같은 많은 단점을 지닌다.

  - 쓰레드에게 인터럽트를 활성/비활성화 하는 _특권(privileged)_ 을 부여해야 한다 : 특권을 가진 쓰레드가 다른 목적으로 사용하지 않음을 신뢰할 수 있어야 한다. 다른 목적으로 사용할 경우 프로세서 독점 및 악의적인 프로그램이 호출되는 등의 문제를 야기할 수 있다.
  
  - 멀티프로세서에 적용할 수 없다 : 다중 쓰레드가 다중 CPU에서 실행 중인 경우, 특정 프로세서에서의 인터럽트 비활성화는 다른 프로세서에 전혀 영향을 주지 않는다. 따라서 다른 프로세스상의 쓰레드들은 critical section 에 동시에 진입할 수 있다.
  
  - 장시간 동안 인터럽트를 중지시키는 것은 중요한 인터럽트 시점을 놓칠 수 있다 : I/O 요청 중 디스크가 읽기 작업을 마쳐서 인터럽트를 보냈으나 이 것을 CPU가 캐치하지 못하게 되면 계속 block 상태에 놓여지게 된다.

  - 비효율적이다 : 인터럽트를 비활성화 시키는 코드들은 최신 CPU에서 느리게 실행되는 경향이 있다.

따라서 인터럽트 비활성화는 매우 제한적으로 사용하여야 한다.

---

## 28.6 Test-And-Set (Atomic Exchange)

![img5](https://user-images.githubusercontent.com/35681772/59991366-1395a580-9682-11e9-9646-6177eed2682a.png)

멀티 프로세서에서는 인터럽트를 중지시키는 것이 의미가 없기 때문에 하드웨어의 지원을 받아 구현되는 mutual exclusion 방법이 제시되었다. 그 중 가장 기본적인 것이 Test-And-Set(Atomic Exchange) 방식이다.

위의 예시를 보면 flag 변수로 락이 구현되어 있다. flag가 켜지면(flag = 1) 락이 사용 중인 상태임을 나타내고 flag가 꺼지면(flag = 0) 사용 가능한 상태이다. 

critical section에 진입하는 첫 쓰레드가 lock()을 호출하여 flag 값이 1인지 **검사(test)** 하고, 처음엔 flag = 0 이라 이 쓰레드가 사용 가능함을 인지하고 flag = 1로 **설정(set)** 하여 해당 쓰레드가 락을 **보유(hold)** 하고 있음을 알린다. critical section에서 나오면 쓰레드가 unlock()을 호출하여 flag = 0 으로 초기화하며 락을 더이상 보유하고있지 않음을 알린다.

첫 번째 쓰레드가 critical section 내에 있을 때 두 번째 쓰레드가 lock()을 호출하면, 두 번째 쓰레드는 while 문으로 **spin-wait** 을 하며 첫 번째 쓰레드가 unlock()을 호출하여 flag = 0이 되기를 기다린다. 그러다 flag = 0이 되면 대기하던 두 번째 쓰레드는 while 문을 빠져나와 flag = 1 로 설정하고 critical section 내로 진입한다.

이 코드의 문제로 아래 두가지가 있다.

  - 정확성(correctness) 문제 : 특정 타이밍에 인터럽트가 발생하면 두 쓰레드 모두 flag를 1로 설정하고 critical section 내에 진입할 수 있다. 아래와 같은 타이밍에 인터럽트가 발생하면 mutual exclusion 제공에 실패하게 된다. 
 
![img7](https://user-images.githubusercontent.com/35681772/59991374-1b554a00-9682-11e9-8d44-46fa1577decb.png)

  - 성능(performance) 문제 : **spin-wait** 방법은 다른 쓰레드가 락을 해제할 때까지 시간을 낭비한다. 이 방법은 특히 단일 프로세서에서 락을 소유한 쓰레드조차 실행할 수 없어 매우 비용이 크다.

---

## 28.7 작동하는 스핀 락의 구현 (Building Working Spin Locks with Test-And-Set)

하드웨어를 이용한 락 기법으로 SPARC 의 ldstub(x86에서 xchg)은 스핀 락 구현을 위한 명령어(어셈블리 명령어)를 제공한다.

![img8](https://user-images.githubusercontent.com/35681772/59991381-20b29480-9682-11e9-9bdd-2d44e5e4c510.png)

![img9](https://user-images.githubusercontent.com/35681772/59991387-260fdf00-9682-11e9-8477-9bab586e18ed.png)

여기서 TestAndSet 함수는 이렇게 동작한다. ptr이 가리키고 있던 예전의 값을 리턴하지만, 새로운 값이 저장되게 되는 방식이다. 

이 동작들이 원자적으로 수행되어 test 하는 동시에 메모리에 새로운 값이 set 된다. 이를 통해 **spin lock** 을 구현할 수 있게 된다.

첫 번째 쓰레드가 처음으로 lock()을 호출하는 상황에서 TestAndSet(flag, 1)을 호출하면 이 루틴은 flag 이전 값인 0을 리턴한다. 하지만 동시에 flag는 1로 set 된다.

이 상태에서 두 번째 쓰레드가 TestAndSet(flag, 1) 루틴을 호출하게 될 경우 리턴되는 값이 1이 된다. 따라서 쓰레드는 0이 리턴될 때 까지 **while문을 spin** 하게 된다. 

첫 번째 쓰레드가 unlock()을 호출하여 flag를 0으로 만들면 그제서야 critical section으로 진입할 수 있게 된다.

락을 획득할 때까지, CPU 사이클을 소모하면서 spin 하는 방식이다. 이 방식을 사용하려면 선점형 스케줄러(preemptive scheduler)를 사용해야 한다. 선점형이 아닌 경우 while문을 spin하며 대기하는 쓰레드가 CPU를 독점할 수 있다.

---

## 28.8 스핀 락 평가 (Evaluating Spin Locks)

따라서 스핀 락을 앞선 평가 기준인 정확성, 공평성, 성능을 기준으로 평가해본다.

  - **정확성(correctness)** : 스핀 락은 mutual exclusion이 제대로 동작하여 임의의 시간에 단 하나의 쓰레드만이 critical section에 진입할 수 있다.

  - **공정성(fairness)** : 스핀 락은 공정성을 보장하지 않는다. while 문을 회전 중인 쓰레드는 경쟁에 밀려서 계속 그 상태에 남아있을 수 있다. 쓰레드가 굶주리는(starvation) 현상을 초래할 수 있다.

  - **성능(performance)**
    - 단일 CPU의 경우 : 스핀 락이 갖는 오버헤드는 상당히 크다. 하나의 쓰레드가 락을 사용하고 있고 다른 쓰레드들도 그 락의 획득을 대기하는 상황에서 스케줄러가 대기중인 쓰레드들을 모두 깨우는 경우, 할당 받은 time slice 동안 while문만 돌며 CPU를 낭비하게 된다.
    - 다중 CPU의 경우 : 스핀은 꽤 합리적으로 작동한다. CPU 간에 독립적으로 스케줄링이 되므로 대기하는 오버헤드가 상대적으로 크지 않게 된다. 다른 프로세서에서 락을 획득하기 위해 while문을 spin 하며 대기하는 것은 그렇게 많은 CPU cycle을 낭비하지는 않으므로 비교적 효율적일 수 있다.

---

## 28.9 Compare-And-Swap

또 다른 하드웨어를 이용한 락 기법으로 SPARC의 Compare-And-Swap(x86에서는 Compare-And-Exchange) 가 있다.

![img10](https://user-images.githubusercontent.com/35681772/59991396-2f00b080-9682-11e9-9456-5f8abc6a1599.png)

Compare-And-Swap 기법의 기본 개념은 ptr이 가리키고 있는 주소의 값이 expected 변수와 일치하는지 검사하는 것이다. 일치 한다면 ptr이 가리키는 주소의 값을 새로운 값으로 갱신하고, 일치하지 않는다면 아무 것도 하지 않아서 원래의 메모리 값을 반환하여 CompareAndSwap을 호출한 코드가 락 획득의 성공 여부를 알 수 있도록 한다.

![img12](https://user-images.githubusercontent.com/35681772/59991407-3627be80-9682-11e9-96ff-f71c09a890bc.png)

위와 같이 Test-And-set 방법과 동일하게 락을 구현할 수 있다. flag 변수가 0인지 검사하고 그렇다면 자동적으로 1로 바꾸어 락을 획득하는 식이다. 한 쓰레드가 락을 보유하고 있을 때, 다른 쓰레드가 락의 획득을 시도하면, 락을 획득할 때까지 while문에서 회전하게 된다.

CompareAndSwap 명령어는 **대기없는 동기화(wait-free synchronization)** 측면에서 TestAndSet 명령어보다 강력하다.

---

## 28.10 Load-linked And Store-conditional

MIPS구조와 같이 critical section 진입 제어 함수를 제작하기 위한 명령어 쌍을 제공하는 플랫폼도 있다.

![img15](https://user-images.githubusercontent.com/35681772/59991414-3f189000-9682-11e9-9275-0a7131cbfc8d.png)

LoadLinked 에서는 일반 로드 명령어와 같이 메모리 값을 레지스터에 저장한다.

StoreConditional 에서는 동일한 주소에 다른 스토어가 없었던 경우에만 저장을 성공한다.

성공한 경우 StoreConditional 은 1을 리턴하고 ptr이 가리키는 값을 value로 갱신한다. 실패하는 경우 곧장 0을 리턴한다.

![img16](https://user-images.githubusercontent.com/35681772/59991419-450e7100-9682-11e9-9766-dee5143ef3ba.png)

load-linked 와 store-conditional 을 사용하여 락을 만들면 위와 같다.

락이 획득 가능한 상태가 되면 쓰레드는 StoreConditional 명령어로 락 획득을 시도하고, 만약 성공하면 쓰레드는 flag 값을 1로 변경하고 critical section 내로 진입한다.

이 방식은 오직 하나의 쓰레드만 flag 값을 1로 설정한 후 락을 획득할 수 있도록 한다.

![img17](https://user-images.githubusercontent.com/35681772/59991425-4a6bbb80-9682-11e9-8a06-8242404205f2.png)

위와 같은 방식으로 코드를 줄일 수 있다.

---

## 28.11 Fetch-And-Add

마지막 하드웨어 기반 기법은 Fetch-And-Add 명령어를 사용하여 원자적으로 특정 주소의 예전 값을 리턴하면서 값을 증가시킨다.

![img18](https://user-images.githubusercontent.com/35681772/59991433-50619c80-9682-11e9-9490-6613d0743fa0.png)

![img19](https://user-images.githubusercontent.com/35681772/59991439-5788aa80-9682-11e9-8235-ba3ced9ff33c.png)

이 방법에서는 하나의 변수만을 사용하지 않고 ticket 과 turn 이라는 조합을 사용하여 락을 만든다.

하나의 쓰레드가 락 획득을 시도하면, ticket 변수에 원자적으로 fetch-and-add 명령어를 실행한다. 그 결과값은 해당 쓰레드의 차례(turn) 을 나타낸다. 

따라서 특정 쓰레드가 자기 차례가 되었을 때(myturn == turn), critical section에 진입하게 되는 구조이다.

이렇게 하면 모든 쓰레드들이 각자의 순서에 따라 진행되기 때문에, starvation 을 방지할 수 있다.

쓰레드가 ticket 값을 할당 받았다는 것은 미래의 어느 때에 실행되기 위해 스케줄 되어 있다는 것이 된다.

---

## 28.12 요약 : 과도한 스핀 (Too Much Spinning : What Now?)

한 프로세서의 두 개의 쓰레드가 락을 획득하기 위해 경쟁하는 경우, 획득을 대기중인 쓰레드는 타이머 인터럽트 전까지 내내 락을 대기하며 스핀만 할 수도 있다. 

만약 N개의 쓰레드가 하나의 락을 획득하기 위해 경쟁한다면, N-1 개의 쓰레드가 할당된 CPU 시간 즉, time slice 내내 스핀만 돌게 된다. 쓰레드가 스핀 구문을 실행하면서 시간만 낭비하고 있는 상황이 발생한다. 

이것을 개선해보자.

---

## 28.13 간단한 접근법 : 무조건 양보! (A Simple Approach : Just Yield, Baby)

![img23](https://user-images.githubusercontent.com/35681772/59991446-61aaa900-9682-11e9-95e9-c4f8a1ffe4d7.png)

하나의 프로세서에서 context switch 가 일어나 쓰레드가 실행이 되었지만, 이전 쓰레드가 인터럽트에 걸리기 전에 락을 이미 획득한 상태라면 현재 쓰레드는 그 락이 해제되기를 기다리며 스핀만 무한히 하는 경우가 발생한다. 어짜피 해당 time-slice 동안 락을 획득한 쓰레드로의 context switch가 일어나지는 않기 때문에, 바뀌지 않을 락을 기다리며 공회전을 하는셈이 된다. 따라서 이런 경우를 어떻게 해결할지??

그 방법론중 하나가 '무조건 양보(Just Yield, Baby)' 이다.

만약 쓰레드가 lock()을 호출하였지만 다른 쓰레드가 락을 보유한 상황이라면, 해당 lock()을 호출한 쓰레드는 갖고 있던 CPU 시간을 양보하여 조금이라도 더 빠르게 락을 보유한 쓰레드가 락을 반납하고 자신이 락을 갖도록 하겠다는 것이다.
 
라운드 로빈(round-robin) 스케줄러를 사용하는 경우라면 락을 갖고 있는 쓰레드가 다시 실행되기까지 100개의 쓰레드 중 99개의 쓰레드가 실행 뒤 양보 를 반복하게 된다.

이 방법 역시 엄청난 context switch 비용을 유발하여 낭비가 심하다. 또한 어떤 쓰레드는 무한히 양보만 하여 starvation 문제도 발생할 수 있다.

---

## 28.14 큐의 사용 : 스핀 대신 잠자기 (Using Queues : Sleeping Instead Of Spinning)

이전 방법들에서는 너무 많은 부분을 운에 맡겨 해결하려 했다. 가령 스케줄러가 안 좋은 선택을 하면 실행되는 쓰레드는 회전을 하며 대기하거나, 즉시 양보하였다. 이 둘 다 낭비의 여지가 있고 쓰레드의 starvation을 차단하지 못한다.

따라서 어떤 쓰레드가 다음으로 락을 획득할 지를 명시적으로 제어할 수 있어야 한다. 이를 위해 운영체제로부터 적절한 지원과, 큐를 이용한 대기 쓰레드들의 관리가 필요하다.

Solaris 방식은 다음과 같다. park()는 호출하는 쓰레드를 잠재우는 함수이고, unpark(threadID)는 threadID로 명시된 특정 쓰레드를 깨우는 함수이다.

이 두 루틴을 사용하여 이미 사용 중인 락을 획득하려는 쓰레드가 있을 경우, 이 쓰레드를 재우고 해당 락이 해제되면 깨우도록 할 수 있다.

또한 아래 개념들을 접목하여 더 강력한 락을 구현하였다.

  - Test-And-Set 개념을 락 대기자 전용 큐와 함께 사용하여 좀 더 효율적인 락을 만든다.
  - 큐를 사용하여 다음으로 락을 획득할 대상을 제어하여 starvation 현상을 피할 수 있도록 한다.

![img21](https://user-images.githubusercontent.com/35681772/59991453-696a4d80-9682-11e9-9769-5e98cfd8438a.png)

위의 예시에서, **guard 변수는 flag와 큐의 삽입 및 삭제 동작을 스핀 락으로 보호하는 데 사용** 하였다. 이 방법은 회전 대기를 완전히 배제하지는 못했지만, 회전 대기 시간이 짧아 합리적이다.

lock() 내에도 추가된 동작이 있다. 한 쓰레드가 lock()을 호출하였는데, 락이 이미 사용중인 경우 자신의 threadID를 얻어(gettid()) 락 대기자 큐에 자신의 tid를 추가하고 guard 변수를 0으로 설정한 뒤 CPU를 양보한다.

여기서 park() 직전에도 경쟁 조건이 발생할 수 있다. 한 쓰레드가 락이 사용중이라 park()문을 수행하기 직전에 context switch가 일어나 락을 소유하고 있는 쓰레드로 CPU가 옮겨갔고, 락을 소유하고 있는 쓰레드는 락을 반납하고 다시 context switch가 일어났다고 가정해보자. park()문 수행 직전인 쓰레드에게 간 CPU는, 락이 반납되어 현재 available 상태이지만 park()문이 실행된다. 이렇게 될 경우 잠재적으로 깨어날 방법이 없다.

이러한 문제를 wakeup/wating race 라고 부르는데, 이 문제를 해결하기위해 Solaris는 setpark() 시스템 콜을 사용하여 해결하였다.

이 setpark() 루틴은 park() 를 호출하기 직전이라는 것을 나타낸다. 


---

## 28.15 다른 운영체제, 다른 지원 (Different OS, Different Support)

Linux 의 경우 **futex** 라는 것을 지원한다. futex는 특정 물리 메모리 주소와 연결이 되어있고 futex마다 커널 내부의 큐를 가지고 있어, 호출자는 futex를 호출하여 필요에 따라 잠을 자거나 깨어날 수 있다.

![img24](https://user-images.githubusercontent.com/35681772/59991460-70915b80-9682-11e9-871d-8d2dcf9d4be0.png)

futex에는 두 개의 명령어가 제공된다.
  - futex_wait(address, expected) : address 값과 expected 값이 동일한 경우 쓰레드를 잠재운다. 같지 않다면 리턴한다.
  - futex_wake(address) : 큐에서 대기하고 있는 쓰레드 하나를 깨운다.

실제 사용되는 락의 동작 메커니즘은 npt1 라이브러리의 lowlevellock.h(gnu libc 라이브러리의 일부) 와 가깝다. 하나의 정수를 이용하여 락의 사용중 여부(최상위 비트를 사용하여)와 대기자 수를 표현한다(나머지 비트를 이용). 락이 음수인 경우 락이 사용중인 셈이다.

---

## 28.16 2단계 락 (Two-Phase Locks)

2단계 락(Two-phase lock)의 경우 락이 곧 해제될 것 같은 경우라면 회전 대기가 유용할 수 있다는 것에서 착안한 방법이다.

첫 번째 단계에선 곧 락을 획득할 수 있을 것이라는 기대로 회전하며 기다린다. 

첫 회전 단계에서 락을 획득하지 못했다면 두 번째 단계로 진입하여 호출자는 sleep 상태로 들어가고 락이 해제된 후에 깨어나도록 한다.

일반화된 방법은 futex가 잠재우기 전에 일정 시간 동안 반복문 내에서 회전하도록 하는 것이다. 

이 방법이 좋은지 나쁜지의 여부는 적용되는 환경에 따라 다르다.

---
## 28.17 요약

하드웨어의 지원(atomic한 명령어의 형태)과 운영체제의 지원(park(), unpark() 등)을 받아 락을 구현하는 방법에 대해 알아보았다.

하지만 실제 사용되는 락은 그 실행하는 방법이나 세부적인 구현 방법이 더 깊으므로 직접 찾아봐야 한다.

---